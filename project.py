# -*- coding: utf-8 -*-

# =================================================================================
# TEK DOSYADA BÄ°RLEÅTÄ°RÄ°LMÄ°Å PROJE KODU
# (TMDB 5000 Movies veri seti Ã¼zerinde RAG uygulamasÄ± - Uygulama Ã§Ä±ktÄ±larÄ± Ä°ngilizce)
#
# Bu dosya; veri yÃ¼kleme, belge hazÄ±rlama, vektÃ¶r veritabanÄ± oluÅŸturma,
# Haystack RAG pipeline kurulumu ve Streamlit arayÃ¼zÃ¼nÃ¼ tek bir yerde toplar.
# =================================================================================

import os
import json
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from datasets import load_dataset

# Haystack ve Gerekli BileÅŸenlerin Ä°Ã§e AktarÄ±mÄ±
from haystack import Pipeline, component
from haystack.dataclasses import Document, ChatMessage
from haystack.document_stores.in_memory import InMemoryDocumentStore
from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder
from haystack.components.writers import DocumentWriter
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.components.builders import PromptBuilder
from haystack_integrations.components.generators.google_genai import GoogleGenAIChatGenerator
from haystack.utils import Secret
from rapidfuzz import fuzz, process

# --- Ã–zel BileÅŸen: Ä°stemi (prompt) ChatMessage'a Ã§evir ---
@component
class PromptToChatMessages:
    """Verilen metin istemini (prompt) tek bir ChatMessage listesine Ã§evirir.

    AmaÃ§: Google GenAI jeneratÃ¶rÃ¼ mesaj listesi bekler; bu nedenle dÃ¼z metin
    istemi ChatMessage nesnesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.

    Girdi: prompt (str)
    Ã‡Ä±ktÄ±: messages (list[ChatMessage]) â€” tek kullanÄ±cÄ± mesajÄ± iÃ§eren liste
    """

    @component.output_types(messages=list[ChatMessage])
    def run(self, prompt: str):
        """KullanÄ±cÄ± mesajÄ± olarak tek bir ChatMessage dÃ¶ndÃ¼rÃ¼r."""
        return {"messages": [ChatMessage.from_user(prompt)]}

# --- YardÄ±mcÄ±: Sorgu bir toplulaÅŸtÄ±rma/sÄ±ralama isteÄŸi mi? ---
def is_aggregation_query(query: str) -> dict:
    """Sorgunun sayÄ±sal bir kÄ±yas/denge sÄ±ralamasÄ± gerektirip gerektirmediÄŸini tespit eder.

    DÃ¶nen sÃ¶zlÃ¼k anahtarlarÄ±:
    - is_agg: bool, gerÃ§ekten bir toplulaÅŸtÄ±rma/sÄ±ralama sorgusu mu?
    - metric: 'rating' | 'popularity' | 'runtime' | 'release_date'
    - order: 'asc' | 'desc'
    - genre: (varsa) tÃ¼r filtresi, Ã¶r. "Action"
    - limit: kaÃ§ sonuÃ§ listelenecek (varsayÄ±lan 5)
    
    Girdi: query (str)
    Ã‡Ä±ktÄ±: dict (is_agg, metric, order, genre, limit)
    """
    query_lower = query.lower()
    
    result = {
        'is_agg': False,
        'metric': None,
        'order': 'desc',  # default: highest/most
        'genre': None,
        'limit': 5
    }
    
    # ToplulaÅŸtÄ±rma (aggregation) anahtar kelimelerini yakala
    agg_keywords = {
        'highest': ('rating', 'desc'),
        'best': ('rating', 'desc'),
        'top rated': ('rating', 'desc'),
        'highest rated': ('rating', 'desc'),
        'most popular': ('popularity', 'desc'),
        'popular': ('popularity', 'desc'),
        'longest': ('runtime', 'desc'),
        'shortest': ('runtime', 'asc'),
        'newest': ('release_date', 'desc'),
        'latest': ('release_date', 'desc'),
        'oldest': ('release_date', 'asc'),
        'lowest rated': ('rating', 'asc'),
        'worst': ('rating', 'asc'),
    }
    
    for keyword, (metric, order) in agg_keywords.items():
        if keyword in query_lower:
            result['is_agg'] = True
            result['metric'] = metric
            result['order'] = order
            break
    
    # Sorguda tÃ¼r (genre) geÃ§iyorsa yakala
    genres = ['action', 'comedy', 'drama', 'thriller', 'horror', 'romance', 
              'sci-fi', 'science fiction', 'fantasy', 'animation', 'documentary',
              'adventure', 'crime', 'mystery', 'war', 'western', 'musical']
    
    for genre in genres:
        if genre in query_lower:
            result['genre'] = genre.replace('sci-fi', 'science fiction').title()
            break
    
    # KaÃ§ sonuÃ§ istendiÄŸini (limit) tespit et
    import re
    numbers = re.findall(r'\b(\d+)\b', query_lower)
    if numbers:
        try:
            limit = int(numbers[0])
            if 1 <= limit <= 20:
                result['limit'] = limit
        except:
            pass
    
    # YaygÄ±n desenler: "top 5", "first 3" vb.
    top_match = re.search(r'(?:top|first)\s+(\d+)', query_lower)
    if top_match:
        result['limit'] = int(top_match.group(1))
    
    return result

# --- YardÄ±mcÄ±: DataFrame Ã¼zerinden toplulaÅŸtÄ±rÄ±lmÄ±ÅŸ sonuÃ§ Ã¼ret ---
def get_aggregated_results(df: pd.DataFrame, agg_params: dict) -> str:
    """DataFrame Ã¼zerinde toplulaÅŸtÄ±rma/sÄ±ralama yapÄ±p biÃ§imlendirilmiÅŸ sonuÃ§ dÃ¶ndÃ¼rÃ¼r.

    Not: "rating" metriÄŸi iÃ§in en az 100 oy ÅŸartÄ± uygulanÄ±r; bu sayede tek tÃ¼k
    oylarla ÅŸiÅŸmiÅŸ filmler liste baÅŸÄ±na gelmez.

    Girdi: df (pd.DataFrame), agg_params (dict)
    Ã‡Ä±ktÄ±: SonuÃ§ metni (str) veya uygun deÄŸilse None
    """
    if df is None or not agg_params['is_agg']:
        return None
    
    metric = agg_params['metric']
    order = agg_params['order']
    genre = agg_params['genre']
    limit = agg_params['limit']
    
    # Ä°stenen metriÄŸi ilgili sÃ¼tun adÄ±na eÅŸle
    metric_col_map = {
        'rating': 'vote_average',
        'popularity': 'popularity',
        'runtime': 'runtime',
        'release_date': 'year'
    }
    
    col = metric_col_map.get(metric)
    if col not in df.columns:
        return None
    
    # TÃ¼r filtresi istenmiÅŸse uygula (genres_norm: normalize edilmiÅŸ tÃ¼r listesi)
    working_df = df.copy()
    if genre:
    # genres_norm bir liste sÃ¼tunudur
        working_df = working_df[working_df['genres_norm'].apply(
            lambda g: any(genre.lower() in str(x).lower() for x in g)
        )]
    
    # Ä°lgili metrik sÃ¼tununda NaN olanlarÄ± Ã§Ä±kar
    working_df = working_df[working_df[col].notna()]
    
    # Ã–NEMLÄ°: Rating iÃ§in minimum oy sayÄ±sÄ± filtresi uygulayalÄ±m
    if metric == 'rating' and 'vote_count' in working_df.columns:
        working_df['vote_count'] = pd.to_numeric(working_df['vote_count'], errors='coerce')
    # Rating sorgularÄ± iÃ§in en az 100 oy gerektir (daha gÃ¼venilir)
        working_df = working_df[working_df['vote_count'] >= 100]
    
    if len(working_df) == 0:
        return f"No movies found matching the criteria (with sufficient votes)."
    
    # SÄ±ralama yÃ¶nÃ¼nÃ¼ belirle ve sÄ±rala
    ascending = (order == 'asc')
    sorted_df = working_df.sort_values(by=col, ascending=ascending)
    
    # Ä°lk N sonucu al
    top_movies = sorted_df.head(limit)
    
    # SonuÃ§larÄ± kullanÄ±cÄ±ya okunabilir biÃ§imde hazÄ±rla
    results = []
    metric_label = {
        'rating': 'Rating',
        'popularity': 'Popularity',
        'runtime': 'Runtime',
        'release_date': 'Year'
    }
    
    for idx, row in top_movies.iterrows():
        title = row.get('title', 'Unknown')
        value = row[col]
        
        if metric == 'runtime':
            value_str = f"{int(value)} min"
        elif metric == 'rating':
            value_str = f"{value:.1f}/10"
            # Show vote count for transparency
            vote_count = row.get('vote_count', 0)
            if vote_count:
                value_str += f" ({int(vote_count):,} votes)"
        elif metric == 'popularity':
            value_str = f"{value:.1f}"
        else:
            value_str = str(value)
        
    # TÃ¼r ve Ã§Ä±kÄ±ÅŸ yÄ±lÄ±nÄ± dahil et
        year = row.get('year', 'N/A')
        results.append(f"- **{title}** ({year}) - {metric_label[metric]}: {value_str}")
    
    genre_text = f" in {genre}" if genre else ""
    order_text = "highest" if order == "desc" else "lowest"
    
    header = f"Here are the {order_text} {metric_label[metric].lower()} movies{genre_text}:\n\n"
    
    # Rating sorgularÄ± iÃ§in not ekle (gÃ¼venilirlik)
    if metric == 'rating':
        header += "*Note: Showing movies with at least 100 votes for reliability.*\n\n"
    
    return header + "\n".join(results)

# --- YardÄ±mcÄ±: Sorguyu (baÅŸlÄ±k) bulanÄ±k eÅŸleÅŸtirme ile normalize et ---
def normalize_query(query: str, movie_titles: list[str]) -> str:
    """Sorguyu (kullanÄ±cÄ± giriÅŸi) bilinen film baÅŸlÄ±klarÄ±na gÃ¶re bulanÄ±k eÅŸleÅŸtirerek gÃ¼Ã§lendirir.

    AmaÃ§: YazÄ±m hatalarÄ±/kÃ¼Ã§Ã¼k-bÃ¼yÃ¼k harf farkÄ± gibi durumlarda doÄŸru filmi bulma
    ÅŸansÄ±nÄ± artÄ±rmak iÃ§in, en yakÄ±n eÅŸleÅŸen baÅŸlÄ±klarÄ± sorguya eklemek.

    Girdi: query (str), movie_titles (list[str])
    Ã‡Ä±ktÄ±: GeliÅŸtirilmiÅŸ/normalize edilmiÅŸ sorgu (str)
    """
    # OlasÄ± film baÅŸlÄ±k adaylarÄ±nÄ± Ã§Ä±kar (muhtemel baÅŸlÄ±k kelime gruplarÄ±)
    words = query.split()
    potential_titles = []
    
    # BÃ¼yÃ¼k harfle baÅŸlayan dizileri veya tÄ±rnak iÃ§i metinleri yakala
    current_title = []
    for word in words:
        clean_word = word.strip('",.:;!?()[]{}')
        if clean_word and (clean_word[0].isupper() or clean_word.lower() in ['the', 'a', 'an', 'of', 'and']):
            current_title.append(clean_word)
        else:
            if len(current_title) >= 1:  # En az 1 kelime
                potential_titles.append(' '.join(current_title))
            current_title = []
    if current_title:
        potential_titles.append(' '.join(current_title))
    
    # TÃ¼m sorguyu da baÅŸlÄ±k adayÄ± olarak dene
    potential_titles.append(query)
    
    # Bilinen baÅŸlÄ±k listesine karÅŸÄ± bulanÄ±k eÅŸleÅŸtirme uygula
    best_matches = []
    for potential in potential_titles:
        if len(potential) >= 3:  # Minimum 3 karakter
            matches = process.extract(
                potential, 
                movie_titles, 
                scorer=fuzz.token_sort_ratio,
                limit=2,
                score_cutoff=75  # Daha iyi eÅŸleÅŸmeler iÃ§in daha yÃ¼ksek eÅŸik
            )
            if matches:
                best_matches.extend([match[0] for match in matches])
    
    # EÅŸleÅŸen baÅŸlÄ±klarla sorguyu zenginleÅŸtir
    if best_matches:
        unique_matches = list(dict.fromkeys(best_matches[:2]))  # En iyi 2 eÅŸleÅŸme
        # Geri getirmeyi (retrieval) iyileÅŸtirmek iÃ§in tam baÅŸlÄ±klarÄ± ekle
        enhanced = f"{query}. Searching for movies: {', '.join(unique_matches)}"
        return enhanced
    
    return query



# --- AdÄ±m 1: Ortam DeÄŸiÅŸkenlerini ve API AnahtarÄ±nÄ± YÃ¼kleme ---
# .env dosyasÄ±nÄ± okuyarak Google API anahtarÄ±nÄ± gÃ¼venli ÅŸekilde alÄ±rÄ±z.
# Hugging Face Spaces'e daÄŸÄ±tÄ±rken bu anahtarÄ± "Secrets" bÃ¶lÃ¼mÃ¼ne ekleyin.
try:
    load_dotenv()
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    HF_TOKEN = os.getenv("HF_TOKEN")
    if not GOOGLE_API_KEY:
        st.error("GOOGLE_API_KEY not found. Please check your .env file or Streamlit secrets settings.")
        st.stop()
except Exception as e:
    st.error(f"Error loading environment variables: {e}")
    st.stop()

# --- AdÄ±m 2: Veri YÃ¼kleme ve HazÄ±rlama (TMDB 5000 Movies) ---
# Bu fonksiyon Hugging Face'ten TMDB 5000 Movies veri setini indirir, iÅŸler ve
# Haystack'in Document formatÄ±na Ã§evirir. Ä°Ã§erik; film adÄ±, Ã¶zet, tÃ¼rler, anahtar
# kelimeler, oyuncu/ekip bilgileri ile zenginleÅŸtirilir. Streamlit cache mekanizmasÄ±
# sayesinde ilk Ã§alÄ±ÅŸtÄ±rmadan sonra tekrar indirilmez.
@st.cache_resource
def load_and_prepare_data():
    """TMDB 5000 Movies veri setini indirir ve Haystack Document listesi Ã¼retir."""
    with st.spinner("Loading TMDB 5000 Movies dataset..."):
        try:
            # 1) Veri setini yÃ¼kle
            if HF_TOKEN:
                dataset = load_dataset("AiresPucrs/tmdb-5000-movies", split="train", token=HF_TOKEN)
            else:
                dataset = load_dataset("AiresPucrs/tmdb-5000-movies", split="train")
            df = dataset.to_pandas()

            # YardÄ±mcÄ±: Liste/JSON tipindeki alanlardan gÃ¼venli ÅŸekilde 'name' deÄŸerlerini Ã§Ä±kar
            def _ensure_list(value):
                if value is None or (isinstance(value, float) and pd.isna(value)):
                    return []
                if isinstance(value, list):
                    return value
                if isinstance(value, str):
                    v = value.strip()
                    # JSON dizesi olabilir
                    if (v.startswith("[") and v.endswith("]")) or (v.startswith("{") and v.endswith("}")):
                        try:
                            parsed = json.loads(v)
                            return parsed if isinstance(parsed, list) else [parsed]
                        except Exception:
                            return []
                    return []
                return []

            def _names_from(field_val):
                items = _ensure_list(field_val)
                names = []
                for it in items:
                    if isinstance(it, dict) and 'name' in it:
                        names.append(str(it['name']))
                return names

            def _cast_top_k(cast_val, k=5):
                items = _ensure_list(cast_val)
                result = []
                for it in items[:k]:
                    if isinstance(it, dict):
                        nm = str(it.get('name') or it.get('original_name') or "")
                        ch = str(it.get('character') or "")
                        if nm and ch:
                            result.append(f"{nm} ({ch})")
                        elif nm:
                            result.append(nm)
                return result

            def _crew_roles(crew_val, roles=("Director", "Screenplay", "Writer"), k=3):
                items = _ensure_list(crew_val)
                result = []
                for it in items:
                    if isinstance(it, dict):
                        job = str(it.get('job') or "")
                        if job in roles:
                            nm = str(it.get('name') or it.get('original_name') or "")
                            if nm:
                                result.append(f"{job}: {nm}")
                return result[:k]

            # Haystack Document nesnelerini oluÅŸtur
            documents = []
            for _, row in df.iterrows():
                title = str(row.get('title') or row.get('original_title') or "")
                overview = str(row.get('overview') or "")
                tagline = str(row.get('tagline') or "")
                release_date = str(row.get('release_date') or "")
                original_language = str(row.get('original_language') or "")
                original_title = str(row.get('original_title') or "")
                genres = _names_from(row.get('genres'))
                keywords = _names_from(row.get('keywords'))
                prod_companies = _names_from(row.get('production_companies'))
                prod_countries = _names_from(row.get('production_countries'))
                spoken_languages = _names_from(row.get('spoken_languages'))
                cast_list = _cast_top_k(row.get('cast'), k=5)
                crew_info = _crew_roles(row.get('crew'))
                runtime = row.get('runtime')
                vote_average = row.get('vote_average')
                popularity = row.get('popularity')

                # Ä°Ã§erik metni (LLM iÃ§in tamamen Ä°ngilizce tutulur)
                parts = []
                header = []
                if title:
                    header.append(f"Title: {title}")
                if original_title and original_title != title:
                    header.append(f"Original Title: {original_title}")
                if header:
                    parts.append(" | ".join(header))
                
                # Aramada gÃ¼Ã§lendirme iÃ§in baÅŸlÄ±ÄŸa farklÄ± yazÄ±m varyasyonlarÄ±nÄ± da ekle
                if title:
                    parts.append(f"Movie Title: {title}")
                    parts.append(f"TITLE: {title.upper()}")  # Uppercase version
                    parts.append(f"title: {title.lower()}")  # Lowercase version
                if original_title and original_title != title:
                    parts.append(f"Original Title: {original_title}")
                    parts.append(f"Alternative Title: {original_title}")
                    
                if overview:
                    parts.append(f"Overview: {overview}")
                if tagline:
                    parts.append(f"Tagline: {tagline}")
                if genres:
                    parts.append(f"Genres: {', '.join(genres)}")
                if keywords:
                    parts.append(f"Keywords: {', '.join(keywords[:15])}")
                if cast_list:
                    parts.append(f"Cast: {', '.join(cast_list)}")
                if crew_info:
                    parts.append(f"Crew: {', '.join(crew_info)}")
                if prod_companies:
                    parts.append(f"Production Companies: {', '.join(prod_companies[:10])}")
                if prod_countries:
                    parts.append(f"Countries: {', '.join(prod_countries)}")
                if spoken_languages:
                    parts.append(f"Languages: {', '.join(spoken_languages)}")
                details = []
                if release_date and release_date.lower() != 'nan':
                    details.append(f"Release Date: {release_date}")
                if original_language and original_language.lower() != 'nan':
                    details.append(f"Language: {original_language}")
                if runtime and not (isinstance(runtime, float) and pd.isna(runtime)):
                    details.append(f"Runtime: {int(runtime)} min")
                if vote_average and not (isinstance(vote_average, float) and pd.isna(vote_average)):
                    details.append(f"Rating: {vote_average}")
                if popularity and not (isinstance(popularity, float) and pd.isna(popularity)):
                    details.append(f"Popularity: {popularity}")
                if details:
                    parts.append("Details: " + "; ".join(details))

                content = "\n".join(parts).strip()

                meta = {
                    'title': title if title else "Unknown Movie",
                    'release_date': release_date,
                    'original_language': original_language
                }

                if content:
                    documents.append(Document(content=content, meta=meta))

            # Film kayÄ±tlarÄ± kÄ±sa ve yapÄ±sal; parÃ§alamaya (split) gerek yok
            return documents
        except Exception as e:
            st.error(f"TMDB veri seti yÃ¼klenirken/iÅŸlenirken hata: {e}")
            return None

# Ham DataFrame yÃ¼kleyici: Ä°statistik/toplulaÅŸtÄ±rma (aggregation) sorularÄ± iÃ§in kullanÄ±lÄ±r
@st.cache_resource
def load_tmdb_dataframe():
    """TMDB 5000 Movies veri setini pandas DataFrame olarak yÃ¼kler ve sÃ¼tunlarÄ± normalize eder.

    Ã‡Ä±ktÄ±: Pandas DataFrame (tÃ¼rler normalize edilmiÅŸ, yÄ±l sÃ¼tunu eklenmiÅŸ)
    """
    try:
        if HF_TOKEN:
            dataset = load_dataset("AiresPucrs/tmdb-5000-movies", split="train", token=HF_TOKEN)
        else:
            dataset = load_dataset("AiresPucrs/tmdb-5000-movies", split="train")
        df = dataset.to_pandas()
    # Tip dÃ¶nÃ¼ÅŸÃ¼mleri (sayÄ±sal alanlar)
        for col in ["vote_average", "popularity", "runtime", "vote_count"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        if 'release_date' in df.columns:
            df['release_date_parsed'] = pd.to_datetime(df['release_date'], errors='coerce')
            df['year'] = df['release_date_parsed'].dt.year

    # YardÄ±mcÄ±: JSON string/list'ten name alanlarÄ±nÄ± Ã§Ä±kar ve normalize et
        def _ensure_list(value):
            if value is None or (isinstance(value, float) and pd.isna(value)):
                return []
            if isinstance(value, list):
                return value
            if isinstance(value, str):
                v = value.strip()
                if (v.startswith("[") and v.endswith("]")) or (v.startswith("{") and v.endswith("}")):
                    try:
                        parsed = json.loads(v)
                        return parsed if isinstance(parsed, list) else [parsed]
                    except Exception:
                        return []
                return []
            return []

        def _names_from(v):
            items = _ensure_list(v)
            names = []
            for it in items:
                if isinstance(it, dict) and 'name' in it:
                    names.append(str(it['name']))
            return names

        def _norm_tr(s: str) -> str:
            s = s.lower()
            repl = {
                'Ã§': 'c', 'ÄŸ': 'g', 'Ä±': 'i', 'iÌ‡': 'i', 'Ã¶': 'o', 'ÅŸ': 's', 'Ã¼': 'u',
                'Ã¢': 'a', 'Ãª': 'e', 'Ã®': 'i', 'Ã´': 'o', 'Ã»': 'u'
            }
            for k, v in repl.items():
                s = s.replace(k, v)
            return s

        if 'genres' in df.columns:
            df['genres_norm'] = df['genres'].apply(lambda v: [_norm_tr(n) for n in _names_from(v)])
        else:
            df['genres_norm'] = [[] for _ in range(len(df))]
        return df
    except Exception as e:
        st.warning(f"Ham tablo yÃ¼klenirken hata: {e}")
        return None

# --- BulanÄ±k eÅŸleÅŸme iÃ§in tÃ¼m film baÅŸlÄ±klarÄ±nÄ± Ã§Ä±kar ---
@st.cache_resource
def get_movie_titles(_documents):
    """BulanÄ±k eÅŸleÅŸtirmede kullanÄ±lmak Ã¼zere tÃ¼m film baÅŸlÄ±klarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.

    Girdi: _documents (list[Document])
    Ã‡Ä±ktÄ±: BaÅŸlÄ±k listesi (list[str])
    """
    if not _documents:
        return []
    titles = []
    for doc in _documents:
        if hasattr(doc, 'meta') and 'title' in doc.meta:
            titles.append(doc.meta['title'])
    return titles

# --- AdÄ±m 3: VektÃ¶r VeritabanÄ± OluÅŸturma ---
# HazÄ±rlanan belgeler, SentenceTransformers gÃ¶mme modeliyle vektÃ¶rleÅŸtirilir ve
# InMemoryDocumentStore Ã¼zerinde saklanÄ±r (FAISS benzeri amaca hizmet eder).
# Bu iÅŸlem cache'lenir; tekrar tekrar Ã§alÄ±ÅŸtÄ±rÄ±lmaz.
@st.cache_resource
def create_faiss_index(_split_docs):
    """Verilen belgeler iÃ§in bellek iÃ§i bir DocumentStore oluÅŸturup gÃ¶mmeleri yazar.

    Girdi: _split_docs (list[Document])
    Ã‡Ä±ktÄ±: InMemoryDocumentStore veya None
    """
    if not _split_docs:
        return None
        
    with st.spinner("Creating vector database and processing documents..."):
        try:
            document_store = InMemoryDocumentStore()
            
            # DokÃ¼man tarafÄ± iÃ§in gÃ¼Ã§lÃ¼ bir gÃ¶mme modeli
            doc_embedder = SentenceTransformersDocumentEmbedder(
                model="sentence-transformers/all-mpnet-base-v2"
            )

            # Belgeleri ve gÃ¶mmeleri saklamaya yazacak kÃ¼Ã§Ã¼k bir indeksleme hattÄ± (pipeline)
            indexing_pipeline = Pipeline()
            indexing_pipeline.add_component("embedder", doc_embedder)
            indexing_pipeline.add_component("writer", DocumentWriter(document_store=document_store))
            indexing_pipeline.connect("embedder.documents", "writer.documents")

            # Ä°ndeksi oluÅŸturmak iÃ§in hattÄ± Ã§alÄ±ÅŸtÄ±r
            indexing_pipeline.run({"embedder": {"documents": _split_docs}})
            
            return document_store
        except Exception as e:
            st.error(f"VektÃ¶r indeksi oluÅŸturulurken hata: {e}")
            return None

# --- AdÄ±m 4: RAG Pipeline Kurulumu ---
# Retriever + PromptBuilder + PromptToChatMessages + GoogleGenAIChatGenerator bileÅŸenleri
# bir araya getirilerek sorgulanabilir bir Haystack hattÄ± (pipeline) oluÅŸturulur.
@st.cache_resource
def build_rag_pipeline(_document_store):
    """Verilen document_store Ã¼zerinde tam bir RAG hattÄ± (pipeline) kurar.

    Girdi: _document_store (InMemoryDocumentStore)
    Ã‡Ä±ktÄ±: Pipeline veya None
    """
    if not _document_store:
        return None
        
    try:
    # 1. Retriever: Serilerde/evrenlerde birden fazla filmi yakalamak iÃ§in top_k=20
        retriever = InMemoryEmbeddingRetriever(document_store=_document_store, top_k=20)

    # 2. Ä°stem (Prompt) Åablonu (tamamen Ä°ngilizce - Gemini iÃ§in optimize)
        template = """You are a helpful movie assistant. Answer the question based ONLY on the provided movie information below.

IMPORTANT RULES:
- Use ONLY the information from the provided documents below
- If information is not available, say "I don't have enough information about this"
- Answer in English only

SMART RESPONSE FORMATTING:
- If MULTIPLE related movies are found (e.g., Star Wars films, Batman movies):
  * List ALL of them in chronological order
  * For each: Title, Year, Rating, Brief description/genres
  * Format as a numbered list
  
- If asking about ONE specific movie (e.g., "What is Inception about?"):
  * Provide COMPREHENSIVE details:
    - Title and original title
    - Overview/plot summary
    - Genres, Release date, Runtime
    - Rating and popularity
    - Cast and Crew
    - Production details
  * Use organized sections with headers

- For comparison queries (highest rated, most popular, year):
  * List movies with relevant metrics
  * Include title, year, rating

{% for doc in documents %}
--- Movie {{ loop.index }} ---
Title: {{ doc.meta['title'] }}
{{ doc.content }}

{% endfor %}

Question: {{question}}
Answer:"""
    # PromptBuilder iÃ§in gereken deÄŸiÅŸkenleri belirt
        prompt_builder = PromptBuilder(template=template, required_variables=["documents", "question"])

        # 3. ÃœreteÃ§ (Generator) â€” Daha tutarlÄ±/ayrÄ±ntÄ±lÄ± cevaplar iÃ§in Ã¼retim parametreleri
        generation_kwargs = {
            "temperature": 0.3,  # Daha tutarlÄ± ve doÄŸru cevaplar iÃ§in daha dÃ¼ÅŸÃ¼k sÄ±caklÄ±k
            "top_p": 0.85,      # Daha odaklÄ± token seÃ§imi iÃ§in
            "max_output_tokens": 1200,  # AyrÄ±ntÄ±lÄ± film bilgileri iÃ§in artÄ±rÄ±ldÄ±
        }
        generator = GoogleGenAIChatGenerator(
            model="gemini-2.0-flash",  # GÃ¼ncel model
            api_key=Secret.from_env_var("GOOGLE_API_KEY"),
            generation_kwargs=generation_kwargs
        )

        # Sorgular iÃ§in metin gÃ¶mme modeli (dokÃ¼man embedder ile aynÄ± olmalÄ±)
        text_embedder = SentenceTransformersTextEmbedder(
            model="sentence-transformers/all-mpnet-base-v2"
        )

        # Ä°stemi ChatMessage listesine Ã§eviren bileÅŸen
        prompt_to_messages = PromptToChatMessages()

        # 4. RAG hattÄ±nÄ± (pipeline) oluÅŸtur
        rag_pipeline = Pipeline()
        rag_pipeline.add_component("text_embedder", text_embedder)
        rag_pipeline.add_component("retriever", retriever)
        rag_pipeline.add_component("prompt_builder", prompt_builder)
        rag_pipeline.add_component("prompt_to_messages", prompt_to_messages)
        rag_pipeline.add_component("generator", generator)

        # BileÅŸenleri birbirine baÄŸla (veri akÄ±ÅŸÄ±)
        rag_pipeline.connect("text_embedder.embedding", "retriever.query_embedding")
        rag_pipeline.connect("retriever.documents", "prompt_builder.documents")
        rag_pipeline.connect("prompt_builder.prompt", "prompt_to_messages.prompt")
        rag_pipeline.connect("prompt_to_messages.messages", "generator.messages")

        return rag_pipeline
    except Exception as e:
        st.error(f"RAG hattÄ± oluÅŸturulurken hata: {e}")
        return None

# --- AdÄ±m 5: Streamlit Web ArayÃ¼zÃ¼ ---
def main():
    # Sayfa baÅŸlÄ±ÄŸÄ± ve ikon
    st.set_page_config(
        page_title="TMDB Movie Q&A Assistant",
        page_icon="ğŸ¬",
        layout="centered",
        initial_sidebar_state="collapsed"
    )
    
    st.title("ğŸ¬ TMDB Movie Q&A Assistant")
    st.markdown(
        """
        <div style='background-color: #f0f2f6; padding: 0.6rem 1rem; border-radius: 0.5rem; margin-bottom: 0.75rem;'>
        <p style='margin: 0; color: #31333F;'>
        ğŸ“š <strong>Akbank Generative AI Bootcamp</strong> â€” RAG-based movie assistant<br>
        ğŸ¯ Dataset: <code>AiresPucrs/tmdb-5000-movies</code> | ğŸ¤– Generator: Google Gemini
        </p>
        </div>
        """,
        unsafe_allow_html=True
    )
    
    # Ã–rnek sorular (iki kolon)
    with st.expander("ğŸ’¡ Example Questions"):
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("""
            **ğŸ­ Series/Franchise:**
            - Star Wars movies
            - Batman films
            - Harry Potter series
            
            **ğŸ¬ Specific Movie:**
            - Tell me about Inception
            - What is The Dark Knight about?
            """)
        with col2:
            st.markdown("""
            **ğŸ“Š Comparison & Rankings:**
            - Highest-rated action movies
            - Top 5 most popular sci-fi movies
            
            **ğŸ¥ Director & Cast:**
            - Movies by Christopher Nolan
            """)

    # Gerekli bileÅŸenleri yÃ¼kle/cache'le
    split_documents = load_and_prepare_data()
    # (Sidebar kullanÄ±mÄ± kaldÄ±rÄ±ldÄ± â€” daha kompakt ve ortalanmÄ±ÅŸ gÃ¶rÃ¼nÃ¼m iÃ§in)
    
    # BulanÄ±k eÅŸleÅŸme iÃ§in film baÅŸlÄ±klarÄ±nÄ± al
    movie_titles = get_movie_titles(split_documents) if split_documents else []
    
    # ToplulaÅŸtÄ±rma sorgularÄ± iÃ§in DataFrame'i yÃ¼kle
    tmdb_df = load_tmdb_dataframe()
    
    if split_documents:
        document_store = create_faiss_index(split_documents)
        if document_store:
            rag_pipeline = build_rag_pipeline(document_store)
        else:
            rag_pipeline = None
    else:
        rag_pipeline = None

    if not rag_pipeline:
        st.warning("Application failed to start. Please check the error messages.")
        st.stop()

    # Sohbet geÃ§miÅŸini oturum durumunda sakla
    if "messages" not in st.session_state:
        st.session_state.messages = list()

    # Sohbet kontrol Ã§ubuÄŸu: Temizle ve mesaj sayacÄ±
    col_clear, col_count = st.columns([1, 1])
    with col_clear:
        if st.button("ğŸ—‘ï¸ Clear Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    with col_count:
        st.markdown(f"<div style='text-align:right; font-size:1.1em;'>ğŸ’¬ <b>{len(st.session_state.messages) // 2}</b> messages</div>", unsafe_allow_html=True)

    # Ã–nceki mesajlarÄ± gÃ¶ster
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al
    if prompt := st.chat_input("Ask about any movie... (e.g., What is Inception about?)"):
        # KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± geÃ§miÅŸe ekle ve gÃ¶ster
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Bu bir toplulaÅŸtÄ±rma/sÄ±ralama sorgusu mu?
        agg_params = is_aggregation_query(prompt)
        
        if agg_params['is_agg'] and tmdb_df is not None:
            # DataFrame ile toplulaÅŸtÄ±rma sorgusunu ele al
            with st.spinner("ğŸ“Š Analyzing movie data..."):
                response = get_aggregated_results(tmdb_df, agg_params)
                
                if not response:
                    response = "I couldn't perform the requested analysis. Please try rephrasing your question."
                
                # Kenar Ã§ubuÄŸunda (sidebar) tanÄ±lama bilgisi gÃ¶ster
                # AyrÄ±ntÄ±lar gizlendi (sidebar kaldÄ±rÄ±ldÄ±)
        
        else:
            # Normal RAG sorgusunu iÅŸle
            # Daha iyi eÅŸleÅŸme iÃ§in sorguyu normalize et (bulanÄ±k baÅŸlÄ±k eÅŸleÅŸtirme)
            normalized_prompt = normalize_query(prompt, movie_titles) if movie_titles else prompt
            
            # Kenar Ã§ubuÄŸunda iyileÅŸtirilmiÅŸ sorguyu gÃ¶ster
            # Ä°steÄŸe baÄŸlÄ±: Sorgu iyileÅŸtirme mesajlarÄ±nÄ± gÃ¶stermiyoruz (daha kÄ±sa gÃ¶rÃ¼nÃ¼m)

            # RAG hattÄ±nÄ± Ã§alÄ±ÅŸtÄ±r ve yanÄ±tÄ± al
            with st.spinner("ğŸ” Searching movie information and generating answer..."):
                try:
                    result = rag_pipeline.run({
                        "text_embedder": {"text": normalized_prompt},
                        "prompt_builder": {"question": prompt}  # Use original for the question
                    })
                    
                    response = "An error occurred or no response was received."
                    
                    if result and "generator" in result and result["generator"]["replies"]:
                        # ChatMessage nesnesinden metin iÃ§eriÄŸini Ã§Ä±kar
                        chat_message = result["generator"]["replies"][0]
                        
                        # _content: TextContent nesnelerini iÃ§eren bir listedir
                        if hasattr(chat_message, '_content') and chat_message._content:
                            response = chat_message._content[0].text.strip()
                        elif hasattr(chat_message, 'content'):
                            response = chat_message.content.strip()
                        else:
                            response = str(chat_message).strip()
                        
                        # Cevap boÅŸ ya da Ã§ok kÄ±sa ise kullanÄ±cÄ±ya yardÄ±mcÄ± mesaj ver
                        if not response or len(response) < 10:
                            response = "I'm sorry, I couldn't find enough information about your question. Please try a more specific question."
                            
                    # TanÄ±lama bilgisi (opsiyonel - kenar Ã§ubuÄŸunda gÃ¶sterilir)
                    # AlÄ±nan belgeler paneli kaldÄ±rÄ±ldÄ± (daha kÄ±sa gÃ¶rÃ¼nÃ¼m)

                except Exception as e:
                    response = f"âŒ An error occurred while processing your query: {str(e)}\n\nPlease try rephrasing your question."
                    st.error(f"Technical details: {e}")

        # AsistanÄ±n yanÄ±tÄ±nÄ± sohbet geÃ§miÅŸine ekle ve gÃ¶ster
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)

if __name__ == "__main__":
    main()